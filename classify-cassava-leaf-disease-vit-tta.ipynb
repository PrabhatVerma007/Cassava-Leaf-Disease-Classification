{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c090dc",
   "metadata": {
    "papermill": {
     "duration": 0.002905,
     "end_time": "2024-11-22T11:06:58.164500",
     "exception": false,
     "start_time": "2024-11-22T11:06:58.161595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cassava Leaf Disease Classification Using Vision Transformer (ViT)\r\n",
    "\r\n",
    "## Introduction\r\n",
    "\r\n",
    "Cassava is a vital carbohydrate source for millions of households in Sub-Saharan Africa, serving as a cornerstone for food security among smallholder farmers. However, viral diseases significantly threaten cassava yields, leading to substantial economic and nutritional challenges. Traditional disease detection methods rely on visual inspections by agricultural experts, which are often labor-intensive, costly, and inaccessible to many farmers, especially those in remote areas with limited resources. To address these challenges, this project leverages advancements in data science and machine learning to develop an automated system capable of accurately classifying cassava leaf diseases from images. By utilizing a Vision Transformer (ViT) model, the system aims to provide timely and reliable disease diagnostics, empowering farmers to take proactive measures and safeguard their crops against devastating losses.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce85cca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-22T11:06:58.170486Z",
     "iopub.status.busy": "2024-11-22T11:06:58.170091Z",
     "iopub.status.idle": "2024-11-22T11:07:07.390681Z",
     "shell.execute_reply": "2024-11-22T11:07:07.389981Z"
    },
    "papermill": {
     "duration": 9.225955,
     "end_time": "2024-11-22T11:07:07.392749",
     "exception": false,
     "start_time": "2024-11-22T11:06:58.166794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cassava-leaf-disease-classification/sample_submission.csv\n",
      "/kaggle/input/cassava-leaf-disease-classification/label_num_to_disease_map.json\n",
      "/kaggle/input/cassava-leaf-disease-classification/train.csv\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train14-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train13-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train04-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train01-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train08-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train00-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train10-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train02-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train15-1327.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train03-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train11-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train12-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train06-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train09-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train07-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/ld_train05-1338.tfrec\n",
      "/kaggle/input/cassava-leaf-disease-classification/test_tfrecords/ld_test00-1.tfrec\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        # Check if the file extension is not '.jpg'\n",
    "        if not filename.lower().endswith('.jpg'):\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66665a",
   "metadata": {
    "papermill": {
     "duration": 0.001848,
     "end_time": "2024-11-22T11:07:07.396958",
     "exception": false,
     "start_time": "2024-11-22T11:07:07.395110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Methodology\r\n",
    "\r\n",
    "The project employs a comprehensive deep learning pipeline centered around the Vision Transformer (ViT) architecture to classify cassava leaf diseases. The key components of the methodology are outlined below:\r\n",
    "\r\n",
    "### 1. **Configuration and Reproducibility**\r\n",
    "   - **Configuration Class (`Config`)**: Encapsulates all hyperparameters and settings required for model training and inference, including model architecture details, training parameters, data augmentation settings, and device configurations.\r\n",
    "   - **Seed Setting Function (`seed_everything`)**: Ensures reproducibility by setting fixed seeds across various libraries (`random`, `numpy`, `torch`) and configuring PyTorch's backend for deterministic behavior.\r\n",
    "\r\n",
    "### 2. **Data Handling and Preprocessing**\r\n",
    "   - **Dataset Class (`CassavaDataset`)**: Custom PyTorch `Dataset` class responsible for loading and preprocessing images. It handles image path construction, loading images using OpenCV, applying transformations, and managing labels.\r\n",
    "   - **Data Augmentation (`ViTDataTransforms`)**: Utilizes the Albumentations library to apply a series of augmentations tailored for Vision Transformers. Training transformations include random resizing, cropping, flipping, rotation, color adjustments, dropout, and normalization to enhance model generalization.\r\n",
    "\r\n",
    "### 3. **Model Architecture**\r\n",
    "   - **Vision Transformer Model (`CassavaViT`)**: Implements the Vision Transformer (ViT) using the `timm` library. The model is initialized with pre-trained weights and configured to output logits corresponding to the five disease categories (four diseases and one healthy class).\r\n",
    "\r\n",
    "### 4. **Training Strategy**\r\n",
    "   - **Trainer Class (`ViTTrainer`)**: Manages the training and validation processes. It sets up the loss function (`CrossEntropyLoss`), optimizer (`AdamW`), and learning rate scheduler (`CosineAnnealingLR`). The trainer also handles mixed-precision training using PyTorch's Automatic Mixed Precision (AMP) to accelerate computations and reduce memory usage.\r\n",
    "   - **Cross-Validation (`StratifiedKFold`)**: Implements Stratified K-Fold cross-validation to ensure each fold maintains the same class distribution as the entire dataset, enhancing the robustness of the model evaluation.\r\n",
    "   - **Training Loop (`train_model`)**: Orchestrates the training across specified folds and epochs. For each fold, it initializes the datasets and dataloaders, trains the model for the defined number of epochs, validates performance, and saves the best-performing model checkpoints based on validation loss.\r\n",
    "\r\n",
    "### 5. **Error Handling and Optimization**\r\n",
    "   - **Robust Image Loading**: The `CassavaDataset` class includes error handling to manage missing or corrupted images by attempting alternative file extensions and substituting with blank images if necessary.\r\n",
    "   - **Memory Management**: After training each fold, the model and trainer instances are deleted, and CUDA caches are cleared to optimize memory usage, especially when training on GPUs with limited resources.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bde1bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T11:07:07.402648Z",
     "iopub.status.busy": "2024-11-22T11:07:07.402243Z",
     "iopub.status.idle": "2024-11-22T20:10:15.232993Z",
     "shell.execute_reply": "2024-11-22T20:10:15.231858Z"
    },
    "papermill": {
     "duration": 32587.835899,
     "end_time": "2024-11-22T20:10:15.234763",
     "exception": false,
     "start_time": "2024-11-22T11:07:07.398864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (21397, 2)\n",
      "Total training samples: 21397\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "3    0.614946\n",
      "4    0.120437\n",
      "2    0.111511\n",
      "1    0.102304\n",
      "0    0.050802\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample image IDs:\n",
      "0    1000015157.jpg\n",
      "1    1000201771.jpg\n",
      "2     100042118.jpg\n",
      "3    1000723321.jpg\n",
      "4    1000812911.jpg\n",
      "Name: image_id, dtype: object\n",
      "Using device: cuda\n",
      "Mixed precision training: enabled\n",
      "Training fold 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a151019de024c31bd216c11baf42dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2010856856.py:257: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler() if config.fp16 else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2140 [00:00<?, ?it/s]/tmp/ipykernel_23/2010856856.py:272: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():                 # Enable autocasting for mixed precision\n",
      "Training: 100%|██████████| 2140/2140 [16:36<00:00,  2.15it/s, loss=0.307]\n",
      "Validating:   0%|          | 0/268 [00:00<?, ?it/s]/tmp/ipykernel_23/2010856856.py:314: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():                    # Enable autocasting for mixed precision\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8871\n",
      "Valid Loss: 0.6046, Accuracy: 0.7757\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_0\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:35<00:00,  2.15it/s, loss=1.02]\n",
      "Validating: 100%|██████████| 268/268 [01:17<00:00,  3.44it/s, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6102\n",
      "Valid Loss: 0.5364, Accuracy: 0.8065\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_1\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:35<00:00,  2.15it/s, loss=0.756]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5499\n",
      "Valid Loss: 0.4915, Accuracy: 0.8201\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_2\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:37<00:00,  2.15it/s, loss=1.18]\n",
      "Validating: 100%|██████████| 268/268 [01:17<00:00,  3.44it/s, loss=0.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5042\n",
      "Valid Loss: 0.4829, Accuracy: 0.8285\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_3\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:38<00:00,  2.14it/s, loss=0.123]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4661\n",
      "Valid Loss: 0.4590, Accuracy: 0.8280\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_4\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:39<00:00,  2.14it/s, loss=0.0466]\n",
      "Validating: 100%|██████████| 268/268 [01:17<00:00,  3.44it/s, loss=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4209\n",
      "Valid Loss: 0.4122, Accuracy: 0.8533\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_5\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:43<00:00,  2.13it/s, loss=0.229]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3844\n",
      "Valid Loss: 0.4116, Accuracy: 0.8542\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_6\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:45<00:00,  2.13it/s, loss=0.0401]\n",
      "Validating: 100%|██████████| 268/268 [01:17<00:00,  3.44it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3463\n",
      "Valid Loss: 0.3726, Accuracy: 0.8650\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_7\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:47<00:00,  2.12it/s, loss=0.88]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3108\n",
      "Valid Loss: 0.3664, Accuracy: 0.8724\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_8\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:48<00:00,  2.12it/s, loss=0.15]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2888\n",
      "Valid Loss: 0.3635, Accuracy: 0.8750\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_0_9\n",
      "Training fold 2\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:48<00:00,  2.12it/s, loss=0.536]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8187\n",
      "Valid Loss: 0.4760, Accuracy: 0.8343\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_2_0\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:49<00:00,  2.12it/s, loss=0.599]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5752\n",
      "Valid Loss: 0.5081, Accuracy: 0.8147\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:49<00:00,  2.12it/s, loss=0.827]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5351\n",
      "Valid Loss: 0.4407, Accuracy: 0.8427\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_2_2\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.279]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4855\n",
      "Valid Loss: 0.4669, Accuracy: 0.8362\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.114]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4421\n",
      "Valid Loss: 0.4980, Accuracy: 0.8331\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.494]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4111\n",
      "Valid Loss: 0.4278, Accuracy: 0.8530\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_2_5\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.184]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3687\n",
      "Valid Loss: 0.3728, Accuracy: 0.8659\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_2_6\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.112]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3233\n",
      "Valid Loss: 0.3733, Accuracy: 0.8738\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.571]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2908\n",
      "Valid Loss: 0.3603, Accuracy: 0.8771\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_2_8\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.136]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2660\n",
      "Valid Loss: 0.3701, Accuracy: 0.8766\n",
      "Training fold 3\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.344]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8228\n",
      "Valid Loss: 0.5014, Accuracy: 0.8163\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_0\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.0178]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6004\n",
      "Valid Loss: 0.5017, Accuracy: 0.8186\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=2.06]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5351\n",
      "Valid Loss: 0.4756, Accuracy: 0.8343\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_2\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.535]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5036\n",
      "Valid Loss: 0.4839, Accuracy: 0.8294\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.0871]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4617\n",
      "Valid Loss: 0.4417, Accuracy: 0.8444\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_4\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.209]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4165\n",
      "Valid Loss: 0.3962, Accuracy: 0.8607\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_5\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:51<00:00,  2.12it/s, loss=0.587]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3823\n",
      "Valid Loss: 0.3806, Accuracy: 0.8698\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_6\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.173]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3406\n",
      "Valid Loss: 0.3739, Accuracy: 0.8712\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_7\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.791]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.42it/s, loss=0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3053\n",
      "Valid Loss: 0.3567, Accuracy: 0.8766\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_8\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2140/2140 [16:50<00:00,  2.12it/s, loss=0.259]\n",
      "Validating: 100%|██████████| 268/268 [01:18<00:00,  3.43it/s, loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2846\n",
      "Valid Loss: 0.3530, Accuracy: 0.8839\n",
      "Saved checkpoint: /kaggle/working/weights/vit_base_patch16_384_fold_3_9\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import cv2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, RandomResizedCrop, Transpose, HorizontalFlip, VerticalFlip,\n",
    "    ShiftScaleRotate, HueSaturationValue, RandomBrightnessContrast,\n",
    "    CoarseDropout, Normalize, CenterCrop, Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Define paths for data and outputs\n",
    "# ---------------------------------------------------\n",
    "PATHS = {\n",
    "    'TRAIN_CSV': '/kaggle/input/cassava-leaf-disease-classification/train.csv',  # Path to training CSV\n",
    "    'TEST_CSV': '/kaggle/input/cassava-leaf-disease-classification/sample_submission.csv',  # Path to test submission CSV\n",
    "    'DISEASE_MAP': '/kaggle/input/cassava-leaf-disease-classification/label_num_to_disease_map.json',  # Path to disease mapping JSON\n",
    "    'TRAIN_IMAGES': '/kaggle/input/cassava-leaf-disease-classification/train_images',  # Directory containing training images\n",
    "    'TEST_IMAGES': '/kaggle/input/cassava-leaf-disease-classification/test_images',  # Directory containing test images\n",
    "    'OUTPUT': '/kaggle/working/submission.csv',  # Path to save the final submission\n",
    "    'WEIGHTS': '/kaggle/working/weights'  # Directory to save model weights\n",
    "}\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for ViT model training and inference.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Check CUDA availability\n",
    "        self.device_type = 'cuda' if torch.cuda.is_available() else 'cpu'  # Device type\n",
    "        self.device = torch.device(self.device_type)  # PyTorch device\n",
    "        \n",
    "        # Model Configuration\n",
    "        self.model_name: str = 'vit_base_patch16_384'  # Vision Transformer model name\n",
    "        self.image_size: int = 384  # Input image size\n",
    "        self.patch_size: int = 16  # Patch size for ViT\n",
    "        self.hidden_size: int = 768  # Hidden size of the transformer\n",
    "        self.num_heads: int = 12  # Number of attention heads\n",
    "        self.num_layers: int = 12  # Number of transformer layers\n",
    "        self.pretrained: bool = True  # Whether to use pretrained weights\n",
    "        \n",
    "        # Training Configuration\n",
    "        self.seed: int = 719  # Random seed for reproducibility\n",
    "        self.num_epochs: int = 10  # Number of training epochs\n",
    "        self.train_batch_size: int = 8 if self.device_type == 'cuda' else 4  # Training batch size based on device\n",
    "        self.valid_batch_size: int = 16 if self.device_type == 'cuda' else 8  # Validation batch size based on device\n",
    "        self.learning_rate: float = 1e-4  # Learning rate for optimizer\n",
    "        self.weight_decay: float = 0.01  # Weight decay for optimizer\n",
    "        self.num_workers: int = 4 if self.device_type == 'cuda' else 2  # Number of workers for data loading\n",
    "        self.grad_accum_steps: int = 2  # Gradient accumulation steps\n",
    "        \n",
    "        # Mixed Precision\n",
    "        self.fp16: bool = self.device_type == 'cuda'  # Use mixed precision only if CUDA is available\n",
    "        \n",
    "        # Cross Validation\n",
    "        self.num_folds: int = 5  # Number of cross-validation folds\n",
    "        self.tta_steps: int = 3  # Number of Test Time Augmentation steps\n",
    "        self.used_epochs: list = [7, 8, 9]  # Epochs to use for inference\n",
    "        self.used_folds: list = [0, 2, 3]  # Folds to use for inference\n",
    "        \n",
    "        # Normalization\n",
    "        self.mean: list = [0.485, 0.456, 0.406]  # Mean values for normalization (ImageNet)\n",
    "        self.std: list = [0.229, 0.224, 0.225]  # Standard deviation values for normalization (ImageNet)\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    \"\"\"Dataset class for Cassava Leaf Disease Classification.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        data_root: str,\n",
    "        transforms: Optional[Compose] = None,\n",
    "        output_label: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)  # Reset DataFrame index for consistency\n",
    "        self.transforms = transforms         # Data augmentation and preprocessing transforms\n",
    "        self.data_root = Path(data_root)     # Root directory for image data\n",
    "        self.output_label = output_label     # Flag to determine if labels are returned\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)  # Return the total number of samples\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, Optional[int]]:\n",
    "        # Get image ID from the DataFrame\n",
    "        image_id = self.df.iloc[index]['image_id']\n",
    "        \n",
    "        # Ensure image has extension\n",
    "        if not image_id.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_id = f\"{image_id}.jpg\"  # Add .jpg extension if missing\n",
    "        \n",
    "        # Construct image path\n",
    "        image_path = self.data_root / image_id\n",
    "        \n",
    "        try:\n",
    "            image = self._load_image(str(image_path))  # Attempt to load the image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {str(e)}\")  # Print error message\n",
    "            # Return a blank image in case of error\n",
    "            image = np.zeros((384, 384, 3), dtype=np.uint8)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']  # Apply transformations if any\n",
    "        \n",
    "        if self.output_label:\n",
    "            target = self.df.iloc[index]['label']  # Get label if output_label is True\n",
    "            return image, target  # Return image and label\n",
    "        return image  # Return only image for inference\n",
    "    \n",
    "    @staticmethod\n",
    "    def _load_image(path: str) -> np.ndarray:\n",
    "        \"\"\"Load and convert BGR image to RGB.\"\"\"\n",
    "        image = cv2.imread(path)  # Read image using OpenCV\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image at {path}\")  # Raise error if image is not found\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)            # Convert BGR to RGB format\n",
    "\n",
    "def print_dataset_info(train_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print dataset information.\"\"\"\n",
    "    print(f\"Total training samples: {len(train_df)}\")      # Print total number of training samples\n",
    "    print(\"\\nLabel distribution:\")                         # Header for label distribution\n",
    "    print(train_df['label'].value_counts(normalize=True))  # Print normalized label counts\n",
    "    print(\"\\nSample image IDs:\")                           # Header for sample image IDs\n",
    "    print(train_df['image_id'].head())                     # Print first few image IDs\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)                          # Set Python random seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)   # Set environment variable for Python hash seed\n",
    "    np.random.seed(seed)                       # Set NumPy random seed\n",
    "    torch.manual_seed(seed)                    # Set PyTorch random seed\n",
    "    torch.cuda.manual_seed(seed)               # Set CUDA random seed\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = True      # Enable benchmarking for performance\n",
    "\n",
    "class CassavaViT(nn.Module):\n",
    "    \"\"\"Vision Transformer model for Cassava disease classification.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        pretrained: bool = True,\n",
    "        model_name: str = 'vit_base_patch16_384'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,   # Use pretrained weights if True\n",
    "            num_classes=num_classes  # Set number of output classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Forward pass through the ViT model\n",
    "\n",
    "class ViTDataTransforms:\n",
    "    \"\"\"Data augmentation and preprocessing transforms optimized for ViT.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transforms(config: Config) -> Compose:\n",
    "        \"\"\"Return training data augmentation transforms.\"\"\"\n",
    "        return Compose([\n",
    "            RandomResizedCrop(\n",
    "                height=config.image_size,\n",
    "                width=config.image_size,\n",
    "                scale=(0.8, 1.0)\n",
    "            ),  # Randomly crop and resize image\n",
    "            Transpose(p=0.5),       # Randomly transpose image dimensions\n",
    "            HorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "            VerticalFlip(p=0.5),    # Random vertical flip\n",
    "            ShiftScaleRotate(\n",
    "                shift_limit=0.2,\n",
    "                scale_limit=0.2,\n",
    "                rotate_limit=30,\n",
    "                p=0.5\n",
    "            ),  # Random shift, scale, and rotation\n",
    "            HueSaturationValue(\n",
    "                hue_shift_limit=20,\n",
    "                sat_shift_limit=30,\n",
    "                val_shift_limit=20,\n",
    "                p=0.5\n",
    "            ),  # Randomly change hue, saturation, and value\n",
    "            RandomBrightnessContrast(\n",
    "                brightness_limit=0.2,\n",
    "                contrast_limit=0.2,\n",
    "                p=0.5\n",
    "            ),  # Random brightness and contrast adjustments\n",
    "            Normalize(\n",
    "                mean=config.mean,\n",
    "                std=config.std,\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0\n",
    "            ),  # Normalize image with mean and std\n",
    "            CoarseDropout(\n",
    "                max_holes=8,\n",
    "                max_height=config.image_size // 16,\n",
    "                max_width=config.image_size // 16,\n",
    "                min_holes=5,\n",
    "                min_height=config.image_size // 32,\n",
    "                min_width=config.image_size // 32,\n",
    "                fill_value=0,\n",
    "                p=0.5\n",
    "            ),  # Randomly drop large regions of the image\n",
    "            ToTensorV2(p=1.0),  # Convert image to PyTorch tensor\n",
    "        ], p=1.)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_valid_transforms(config: Config) -> Compose:\n",
    "        \"\"\"Return validation data preprocessing transforms.\"\"\"\n",
    "        return Compose([\n",
    "            Resize(config.image_size, config.image_size),  # Resize image to desired size\n",
    "            Normalize(\n",
    "                mean=config.mean,\n",
    "                std=config.std,\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0\n",
    "            ),  # Normalize image with mean and std\n",
    "            ToTensorV2(p=1.0),  # Convert image to PyTorch tensor\n",
    "        ], p=1.)\n",
    "\n",
    "class ViTTrainer:\n",
    "    \"\"\"Trainer class optimized for Vision Transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: Config):\n",
    "        self.model = model                             # Assign the model to an instance variable\n",
    "        self.config = config                           # Store the configuration parameters\n",
    "        self.criterion = nn.CrossEntropyLoss()         # Define the loss function (Cross-Entropy Loss)\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,                   # Set learning rate from config\n",
    "            weight_decay=config.weight_decay           # Apply weight decay regularization\n",
    "        )\n",
    "        \n",
    "        # Cosine annealing scheduler\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=config.num_epochs,                   # Maximum number of iterations (epochs)\n",
    "            eta_min=1e-6                               # Minimum learning rate after decay\n",
    "        )\n",
    "        \n",
    "        # Initialize GradScaler for mixed precision training if CUDA is available\n",
    "        self.scaler = torch.cuda.amp.GradScaler() if config.fp16 else None\n",
    "                                                     # Use GradScaler for mixed precision if fp16 is enabled\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader, device: torch.device) -> float:\n",
    "        \"\"\"Train the model for one epoch.\"\"\"\n",
    "        self.model.train()                                     # Set the model to training mode\n",
    "        total_loss = 0                                         # Initialize total loss for the epoch\n",
    "        \n",
    "        with tqdm(train_loader, desc='Training') as pbar:      # Create a progress bar for the training loop\n",
    "            for batch_idx, (images, targets) in enumerate(pbar):\n",
    "                images = images.to(device)                     # Move images to the specified device (GPU or CPU)\n",
    "                targets = targets.to(device)                   # Move targets to the device\n",
    "                \n",
    "                # Mixed precision training if enabled\n",
    "                if self.config.fp16:\n",
    "                    with torch.cuda.amp.autocast():                 # Enable autocasting for mixed precision\n",
    "                        outputs = self.model(images)                # Forward pass through the model\n",
    "                        loss = self.criterion(outputs, targets)     # Compute loss between outputs and targets\n",
    "                        loss = loss / self.config.grad_accum_steps  # Normalize loss for gradient accumulation\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()         # Backward pass with scaled loss for mixed precision\n",
    "                    \n",
    "                    if (batch_idx + 1) % self.config.grad_accum_steps == 0:\n",
    "                        self.scaler.step(self.optimizer)       # Update model parameters\n",
    "                        self.scaler.update()                   # Update the scaler for next iteration\n",
    "                        self.optimizer.zero_grad()             # Reset gradients\n",
    "                else:\n",
    "                    outputs = self.model(images)               # Forward pass through the model\n",
    "                    loss = self.criterion(outputs, targets)    # Compute loss\n",
    "                    loss = loss / self.config.grad_accum_steps # Normalize loss for gradient accumulation\n",
    "                    \n",
    "                    loss.backward()                            # Backward pass\n",
    "                    \n",
    "                    if (batch_idx + 1) % self.config.grad_accum_steps == 0:\n",
    "                        self.optimizer.step()                  # Update model parameters\n",
    "                        self.optimizer.zero_grad()             # Reset gradients\n",
    "                \n",
    "                total_loss += loss.item() * self.config.grad_accum_steps  # Accumulate total loss\n",
    "                pbar.set_postfix({'loss': loss.item() * self.config.grad_accum_steps})  # Update progress bar with current loss\n",
    "        \n",
    "        self.scheduler.step()                                  # Update learning rate scheduler\n",
    "        return total_loss / len(train_loader)                  # Return average loss for the epoch\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, valid_loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "        self.model.eval()                                      # Set the model to evaluation mode\n",
    "        total_loss = 0                                         # Initialize total loss for the validation\n",
    "        predictions = []                                       # List to store predicted labels\n",
    "        targets = []                                           # List to store true labels\n",
    "        \n",
    "        with tqdm(valid_loader, desc='Validating') as pbar:    # Create a progress bar for the validation loop\n",
    "            for images, batch_targets in pbar:\n",
    "                images = images.to(device)                     # Move images to the specified device\n",
    "                batch_targets = batch_targets.to(device)       # Move targets to the device\n",
    "                \n",
    "                if self.config.fp16:\n",
    "                    with torch.cuda.amp.autocast():                    # Enable autocasting for mixed precision\n",
    "                        outputs = self.model(images)                   # Forward pass through the model\n",
    "                        loss = self.criterion(outputs, batch_targets)  # Compute loss\n",
    "                else:\n",
    "                    outputs = self.model(images)                   # Forward pass through the model\n",
    "                    loss = self.criterion(outputs, batch_targets)  # Compute loss\n",
    "                \n",
    "                total_loss += loss.item()                          # Accumulate total loss\n",
    "                predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())  # Store predicted labels\n",
    "                targets.extend(batch_targets.cpu().numpy())        # Store true labels\n",
    "                \n",
    "                pbar.set_postfix({'loss': loss.item()})            # Update progress bar with current loss\n",
    "        \n",
    "        accuracy = np.mean(np.array(predictions) == np.array(targets))  # Calculate accuracy\n",
    "        return total_loss / len(valid_loader), accuracy            # Return average loss and accuracy\n",
    "\n",
    "def train_model(config: Config, train_df: pd.DataFrame):\n",
    "    \"\"\"Train the ViT model using cross-validation.\"\"\"\n",
    "    print(f\"Using device: {config.device_type}\")  # Inform about the device being used\n",
    "    print(f\"Mixed precision training: {'enabled' if config.fp16 else 'disabled'}\")  # Inform about mixed precision\n",
    "    \n",
    "    # Create Stratified K-Fold splits to maintain class distribution\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=config.num_folds,\n",
    "        shuffle=True,\n",
    "        random_state=config.seed\n",
    "    )\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df.label)):\n",
    "        if fold not in config.used_folds:\n",
    "            continue  # Skip folds that are not used for inference\n",
    "        \n",
    "        print(f'Training fold {fold}')  # Inform about the current fold being trained\n",
    "        \n",
    "        # Split data into training and validation sets based on indices\n",
    "        train_data = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        valid_data = train_df.iloc[valid_idx].reset_index(drop=True)\n",
    "        \n",
    "        # Create training dataset with data augmentation\n",
    "        train_dataset = CassavaDataset(\n",
    "            train_data,\n",
    "            PATHS['TRAIN_IMAGES'],\n",
    "            transforms=ViTDataTransforms.get_train_transforms(config)\n",
    "        )\n",
    "        \n",
    "        # Create validation dataset without data augmentation\n",
    "        valid_dataset = CassavaDataset(\n",
    "            valid_data,\n",
    "            PATHS['TRAIN_IMAGES'],\n",
    "            transforms=ViTDataTransforms.get_valid_transforms(config)\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader for training data\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.train_batch_size,\n",
    "            shuffle=True,  # Shuffle training data\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=True if config.device_type == 'cuda' else False  # Enable pin memory for CUDA\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader for validation data\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=config.valid_batch_size,\n",
    "            shuffle=False,  # Do not shuffle validation data\n",
    "            num_workers=config.num_workers,\n",
    "            pin_memory=True if config.device_type == 'cuda' else False  # Enable pin memory for CUDA\n",
    "        )\n",
    "        \n",
    "        # Initialize the Vision Transformer model and move it to the device\n",
    "        model = CassavaViT(\n",
    "            num_classes=train_df.label.nunique(),  # Number of unique classes\n",
    "            pretrained=config.pretrained,  # Use pretrained weights\n",
    "            model_name=config.model_name  # Specify model architecture\n",
    "        ).to(config.device)\n",
    "        \n",
    "        trainer = ViTTrainer(model, config)  # Initialize the trainer\n",
    "        best_loss = float('inf')  # Initialize best loss for checkpointing\n",
    "        \n",
    "        for epoch in range(config.num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{config.num_epochs}')  # Inform about the current epoch\n",
    "            \n",
    "            train_loss = trainer.train_epoch(train_loader, config.device)  # Train for one epoch\n",
    "            valid_loss, accuracy = trainer.validate(valid_loader, config.device)  # Validate the model\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f}')  # Print training loss\n",
    "            print(f'Valid Loss: {valid_loss:.4f}, Accuracy: {accuracy:.4f}')  # Print validation loss and accuracy\n",
    "            \n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss  # Update best loss if current validation loss is lower\n",
    "                checkpoint_path = os.path.join(\n",
    "                    PATHS['WEIGHTS'],\n",
    "                    f'{config.model_name}_fold_{fold}_{epoch}'\n",
    "                )  # Define checkpoint path\n",
    "                torch.save(model.state_dict(), checkpoint_path)  # Save model weights\n",
    "                print(f'Saved checkpoint: {checkpoint_path}')  # Inform about saved checkpoint\n",
    "        \n",
    "        del model, trainer  # Delete model and trainer to free memory\n",
    "        if config.device_type == 'cuda':\n",
    "            torch.cuda.empty_cache()  # Clear CUDA cache to free up GPU memory\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Create weights directory if it doesn't exist\n",
    "    os.makedirs(PATHS['WEIGHTS'], exist_ok=True)\n",
    "    \n",
    "    config = Config()  # Initialize configuration\n",
    "    seed_everything(config.seed)  # Set random seeds for reproducibility\n",
    "    \n",
    "    # Load training data from CSV\n",
    "    train_df = pd.read_csv(PATHS['TRAIN_CSV'])\n",
    "    print(f\"Training data shape: {train_df.shape}\")  # Print shape of training data\n",
    "    \n",
    "    # Print dataset information\n",
    "    print_dataset_info(train_df)  # Display dataset statistics\n",
    "    \n",
    "    # Train the Vision Transformer model using cross-validation\n",
    "    train_model(config, train_df)\n",
    "    \n",
    "    print(\"Training completed successfully!\")  # Inform that training is complete\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()  # Execute the main function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad613e",
   "metadata": {
    "papermill": {
     "duration": 6.900018,
     "end_time": "2024-11-22T20:10:28.944656",
     "exception": false,
     "start_time": "2024-11-22T20:10:22.044638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\r\n",
    "\r\n",
    "This project demonstrates the effective application of Vision Transformers (ViT) in addressing the critical task of Cassava Leaf Disease Classification. By leveraging a pre-trained ViT model, robust data augmentation strategies, and a meticulous training regimen incorporating cross-validation and mixed-precision training, the system achieves high accuracy in distinguishing between different cassava diseases and identifying healthy leaves. The automated detection system offers a scalable and accessible solution for farmers, enabling timely interventions to prevent crop losses and enhance agricultural productivity. Future work may explore integrating additional data sources, refining model architectures, and deploying the model as a mobile application to facilitate real-time disease diagnostics in resource-constrained environments.\r\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1718836,
     "sourceId": 13836,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32622.857354,
   "end_time": "2024-11-22T20:10:38.524820",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-22T11:06:55.667466",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3b0df4b1b7024b97968e174d0c687465": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "741dc8031fb140c89a17c167019dbed5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a151019de024c31bd216c11baf42dc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a06e2bba1ef5461fb68025b0e5d4ad63",
        "IPY_MODEL_b458bf6504a54d04823ee5e18e4eea02",
        "IPY_MODEL_d38fe5f36eb14244895044007568bb75"
       ],
       "layout": "IPY_MODEL_741dc8031fb140c89a17c167019dbed5"
      }
     },
     "9d30bc508199476ab1a34e9f1f09e5f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a06e2bba1ef5461fb68025b0e5d4ad63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_aca19ef702e2495fb6612a208a542cc4",
       "placeholder": "​",
       "style": "IPY_MODEL_9d30bc508199476ab1a34e9f1f09e5f2",
       "value": "model.safetensors: 100%"
      }
     },
     "ab45ee201524494c900d72150304b17a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aca19ef702e2495fb6612a208a542cc4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aecb036f1a1246c19137eb46fa90cfdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b458bf6504a54d04823ee5e18e4eea02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab45ee201524494c900d72150304b17a",
       "max": 347452074.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3b0df4b1b7024b97968e174d0c687465",
       "value": 347452074.0
      }
     },
     "d38fe5f36eb14244895044007568bb75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_eb1a54ad3f7b49cd8cc2460954b9dfb7",
       "placeholder": "​",
       "style": "IPY_MODEL_aecb036f1a1246c19137eb46fa90cfdc",
       "value": " 347M/347M [00:01&lt;00:00, 244MB/s]"
      }
     },
     "eb1a54ad3f7b49cd8cc2460954b9dfb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
